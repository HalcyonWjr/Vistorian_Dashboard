{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f865f5",
   "metadata": {},
   "source": [
    "# Cleaning for User Timeline Data\n",
    "\n",
    "- Step0: import modules, read files\n",
    "- Step1: get page view, and delete unuseful columns\n",
    "- Step2: match log event names\n",
    "- Step3: process timestamp\n",
    "- Step4: Refine some logs\n",
    "- Step5: get session number\n",
    "- Group Them All and Export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6def42",
   "metadata": {},
   "source": [
    "## Step0: Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d96eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962678f",
   "metadata": {},
   "source": [
    "## Step1: get page view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e929d3",
   "metadata": {},
   "source": [
    "**Find the View depending on log file structure**\n",
    "1. clean_ref(col_name, df) for those with 'referer' column\n",
    "2. assign_view(row) for those without 'referer' column but has 'uuid' (identifier only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97b4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ref(col_name, df):\n",
    "    # Locate the visualization views\n",
    "    df['view'] = df[col_name].apply(lambda x: 'Node-Link' if 'nodelink' in str(x).lower() else\n",
    "                                           'Matrix' if 'matrix' in str(x).lower() else\n",
    "                                           'Timeline' if 'dynamicego' in str(x).lower() else\n",
    "                                           'Coordinated' if 'mat-nl' in str(x).lower() else\n",
    "                                           'Map' if 'map' in str(x).lower() else x)\n",
    "    # Clean the rest and delete unuseful columns\n",
    "    df.loc[df['view'].str.contains('vistorian', case=False), 'view'] = ''\n",
    "    df.drop(columns=[col_name], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74d4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_view(row):\n",
    "    for col in ['action', 'label', 'value']:\n",
    "        cell_value = str(row[col])\n",
    "        if 'nodelink' in cell_value:\n",
    "            return 'Node-Link'\n",
    "        elif 'map' in cell_value:\n",
    "            return 'Map'\n",
    "        elif 'matrix' in cell_value:\n",
    "            return 'Matrix'\n",
    "        elif 'mat-nl' in cell_value:\n",
    "            return 'Coordinated'\n",
    "        elif 'dynamicego' in cell_value:\n",
    "            return 'Timeline'\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bab52e",
   "metadata": {},
   "source": [
    "## Step2: Match log_ID with titles and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c943db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_log_info(df):\n",
    "    # Perform inner join\n",
    "    merged_df = pd.merge(df, df_log, left_on='category', right_on='log_ID')\n",
    "    \n",
    "    # Drop the redundant 'log_ID' column\n",
    "    merged_df = merged_df.drop(columns=['log_ID'])\n",
    "    \n",
    "    # Sort the merged DataFrame by 'ts' in ascending order\n",
    "    merged_df = merged_df.sort_values(by='ts', ascending=True)\n",
    "    merged_df = merged_df.rename(columns={'category': 'logID'})\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1bd3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_log_info(df):\n",
    "    # Append '_click' to the 'log_title' column where 'value' is equal to 'Click', apply to Node and Link logs\n",
    "    df.loc[df['value'] == 'Click', 'log_title'] = df.loc[df['value'] == 'Click', 'log_title'] + '_click'\n",
    "    \n",
    "    # Append labelling type to 'vis_9' 0=automatic/importance, 1=hide all, 2=show all\n",
    "    df.loc[(df['value'] == \"0\") & (df['logID'] == 'vis_9'), 'log_title'] = df.loc[(df['value'] == \"0\") & (df['logID'] == 'vis_9'), 'log_title'] + '-Automatic/importance'\n",
    "    df.loc[(df['value'] == \"1\") & (df['logID'] == 'vis_9'), 'log_title'] = df.loc[(df['value'] == \"1\") & (df['logID'] == 'vis_9'), 'log_title'] + '-Hide All'\n",
    "    df.loc[(df['value'] == \"2\") & (df['logID'] == 'vis_9'), 'log_title'] = df.loc[(df['value'] == \"2\") & (df['logID'] == 'vis_9'), 'log_title'] + '-Show All'\n",
    "    \n",
    "    # Update \"Column Specified\"\n",
    "    if len(df.loc[df['logID'] == 'dat_11']) > 0:\n",
    "        value_to_append = df.loc[df['logID'] == 'dat_11', 'value'].astype(str).values[0]\n",
    "        df.loc[df['logID'] == 'dat_11', 'log_title'] = df.loc[df['logID'] == 'dat_11', 'log_title'].astype(str) + value_to_append\n",
    "\n",
    "    # Drop column 'value'\n",
    "    df.drop(columns=['value'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090f6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_index(df, file_name=None):\n",
    "    log_category_index_list = [\n",
    "        { \"category\": \"data\", \"index\": 0 },\n",
    "        { \"category\": \"visualization\", \"index\": 1 },\n",
    "        { \"category\": \"logging\", \"index\": 2 },\n",
    "        { \"category\": \"help\", \"index\": 3 },\n",
    "        { \"category\": \"communication\", \"index\": 4 },\n",
    "        { \"category\": \"error\", \"index\": 5 },\n",
    "        { \"category\": \"bookmarking\", \"index\": 6 },\n",
    "    ]\n",
    "    \n",
    "    category_to_index = {item[\"category\"]: item[\"index\"] for item in log_category_index_list}\n",
    "    df['log_category_index'] = df['log_category'].map(category_to_index)\n",
    "    \n",
    "    # Find and print the file name and index for the NaN values\n",
    "    if file_name is not None:\n",
    "        nan_indices = df[df['log_category_index'].isna()].index\n",
    "        for index in nan_indices:\n",
    "            print(f\"File: {file_name}, Index: {index}\")\n",
    "    \n",
    "    # Fill NaN values with a default value (e.g., -1) before converting to integer data type\n",
    "    df['log_category_index'] = df['log_category_index'].fillna(-1).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e276a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decluster_rows(df):\n",
    "    removed_logs = ['_trace', 'log', 'log_1', 'log_3', 'log_11', 'log_5', 'log_6', 'log_7', 'log_9','log_10', 'log_11', 'log_13', 'log_18', 'log_14', 'log_16', 'log_17', 'log_18','dat_9', 'dat_12','dat_17', 'dat_18', 'dat_19']\n",
    "    df = df[~df['logID'].isin(removed_logs)]\n",
    "    \n",
    "    # Usually the first log is Visit Page, we should remove that\n",
    "    df = df.iloc[1:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7206d3d0",
   "metadata": {},
   "source": [
    "## Step3: Sort, Combine by Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e688e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_time(df):\n",
    "    # Add a new index column to the DataFrame\n",
    "    df = df.reset_index(drop=True).reset_index()\n",
    "    \n",
    "    # Calculate the time relative to the first timestamp\n",
    "    df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
    "    df['relative_time'] = df['ts'].apply(lambda x: (x - df['ts'].iloc[0]).total_seconds())\n",
    "    \n",
    "    # Drop the original ts column\n",
    "    df.drop(columns=['ts'], inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7862c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_logs(df):\n",
    "    # Get a new column for start time\n",
    "    df = df.assign(start=df['relative_time'])\n",
    "    \n",
    "    # Create an empty DataFrame to store the combined rows\n",
    "    combined_data = pd.DataFrame(columns=[\"index\", \"user\", \"logID\", \"view\", \"log_title\",\n",
    "                                        \"log_category\",\"log_category_index\", \"relative_time\", \"start\", \"end\"])\n",
    "    \n",
    "    # Iterate through the rows of the original DataFrame\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        row = df.iloc[i]\n",
    "        index = row[\"index\"]\n",
    "        user = row[\"user\"]\n",
    "        logID = row[\"logID\"]\n",
    "        view = row[\"view\"]\n",
    "        log_title = row[\"log_title\"]\n",
    "        log_category = row[\"log_category\"]\n",
    "        log_category_index = row[\"log_category_index\"]\n",
    "        relative_time = row[\"relative_time\"]\n",
    "        start = row['start']\n",
    "        end = start + 1\n",
    "#         start = round(row[\"start\"], ndigits=0)\n",
    "#         end = round(start + 1, ndigits=0)\n",
    "\n",
    "        # Check if the current row can be combined with the next row(s)\n",
    "        j = i + 1\n",
    "        while j < len(df):\n",
    "            next_row = df.iloc[j]\n",
    "            if next_row[\"log_title\"] == log_title and next_row[\"start\"] - end < 1:\n",
    "                end = next_row[\"start\"] + 1\n",
    "#                 end = round(end, ndigits=0)\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Add the combined row to the new DataFrame\n",
    "        new_row = pd.DataFrame({\"index\": [index],\n",
    "                                \"user\": [user],\n",
    "                                \"logID\": [logID],\n",
    "                                \"view\": [view],\n",
    "                                \"log_title\": [log_title],\n",
    "                                \"log_category\": [log_category],\n",
    "                                \"log_category_index\": [log_category_index],\n",
    "                                \"relative_time\": [relative_time],\n",
    "                                \"start\": [start],\n",
    "                                \"end\": [end]})\n",
    "        combined_data = pd.concat([combined_data, new_row], ignore_index=True)\n",
    "\n",
    "        # Update the index to continue from the last checked row\n",
    "        i = j\n",
    "    \n",
    "    # Round the start and end to integer\n",
    "    combined_data['start'] = combined_data['start'].astype(int)\n",
    "    combined_data['end'] = combined_data['end'].astype(int)\n",
    "    \n",
    "\n",
    "    # Drop the original ts column\n",
    "    combined_data.drop(columns=['relative_time', 'index'], inplace=True)\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821748a",
   "metadata": {},
   "source": [
    "## Step4: Refine some logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adff2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_logs(df):\n",
    "    # Update Visiting any Vistorian Page to 'Visit Page X'\n",
    "    df['log_title'] = df.apply(lambda row: 'Visit Page ' + row['view'] if row['log_title'] == 'Visiting any Visitorian Page' else row['log_title'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e30d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_vis_view(df):\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    # Replace empty cells with NaN in the 'view' column\n",
    "    df.loc[(df['log_category'] == 'visualization') & (df['view'] == ''), 'view'] = np.nan\n",
    "    \n",
    "    # Forward fill NaN values in the entire 'view' column\n",
    "    df['view'] = df['view'].ffill()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab381d0",
   "metadata": {},
   "source": [
    "## Step5: Is she/he a Multiple-Session user?\n",
    "**Break sessions by 20min inactivity in logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e327cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_session(df):\n",
    "    df['userSession'] = 1\n",
    "    previous_session_end = df.iloc[0]['end']\n",
    "\n",
    "    for index, row in df[1:].iterrows():\n",
    "        if row['start'] - previous_session_end >= 1200:\n",
    "            df.at[index, 'userSession'] = df.at[index - 1, 'userSession'] + 1\n",
    "        else:\n",
    "            df.at[index, 'userSession'] = df.at[index - 1, 'userSession']\n",
    "\n",
    "        previous_session_end = row['end']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "967d4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_time(df):\n",
    "    unique_sessions = df['userSession'].unique()\n",
    "\n",
    "    for session_num in unique_sessions:\n",
    "        if session_num > 1:\n",
    "            session_start = df[df['userSession'] == session_num].iloc[0]['start']\n",
    "\n",
    "            df.loc[df['userSession'] == session_num, 'start'] -= session_start\n",
    "            df.loc[df['userSession'] == session_num, 'end'] -= session_start\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8682596",
   "metadata": {},
   "source": [
    "## Step6: Loop all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2841dc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   usrID  session                                     event category  index  \\\n",
      "0    187        1  landing first time in any Vistorian page  logging      2   \n",
      "1    187        1                   Support Form Completion  logging      2   \n",
      "\n",
      "  view  start  end  \n",
      "0  NaN    0.0  0.0  \n",
      "1  NaN    0.0  0.0  \n",
      "['ss_Explorer_2461.csv', 'demo_user_187.csv', 'ss_Explorer_2699.csv', 'ms_explorer_2596.csv', 'data_struggler_626.csv', 'ms_Explorer_user_482.csv', 'ss_Explorer_2094.csv', 'data_strugglers_2736.csv', 'demo_2039.csv', 'data_struggler_519.csv', 'ms_explorer_411.csv']\n"
     ]
    }
   ],
   "source": [
    "df_log = pd.read_csv(\"log_categories.csv\")\n",
    "df_old_user_timeline = pd.read_csv(\"0813_test_multi.csv\")\n",
    "\n",
    "print(df_old_user_timeline.head(2))\n",
    "\n",
    "# Set the directory path to your CSV files\n",
    "os.chdir(\"./data\")\n",
    "files = os.listdir()[1:]\n",
    "\n",
    "if '.DS_Store' in files:\n",
    "    files.remove('.DS_Store')\n",
    "    \n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "902e1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_directory = os.getcwd()\n",
    "# print(current_directory)\n",
    "# print(os.system('ls -lr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b12c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # 1. Get page view\n",
    "    if 'uuid' in df.columns:\n",
    "        df['view'] = df.apply(assign_view, axis=1)\n",
    "        df.drop(['action', 'label', 'uuid'], axis=1, inplace=True)\n",
    "    else:\n",
    "        clean_ref('referer', df)\n",
    "        df.drop(['action', 'label', 'ip'], axis=1, inplace=True)\n",
    "\n",
    "    # 2. Match logID with title and category\n",
    "    df = join_log_info(df)\n",
    "    df = update_log_info(df)\n",
    "    df = get_category_index(df)\n",
    "\n",
    "    # 3. Decluster file with unuseful logs\n",
    "    df = decluster_rows(df)\n",
    "\n",
    "    # 4. Calculate relative time and combine consecutive logs\n",
    "    df = relative_time(df)\n",
    "    df = combine_logs(df)\n",
    "\n",
    "\n",
    "    # 5. Refine some log titles\n",
    "    df = refine_logs(df)\n",
    "    df = fill_vis_view(df)\n",
    "\n",
    "    # 6. Get session IDs and reset relative time\n",
    "    df = assign_session(df)\n",
    "    df = reset_time(df)\n",
    "    \n",
    "    # 7. Rename the df to match the Visualization specs\n",
    "    df = df.rename(columns={'user': 'usrID'})\n",
    "    df = df.rename(columns={'log_title': 'event'})\n",
    "    df = df.rename(columns={'log_category': 'category'})\n",
    "    df = df.rename(columns={'log_category_index': 'index'})\n",
    "    \n",
    "\n",
    "    folder_path = \"../export\"\n",
    "    full_path = f\"{folder_path}/{file}\"\n",
    "    \n",
    "    df.to_csv(full_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f2064cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/halcyon/Desktop/vistorian dashboard/user_timeline/data\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"/Users/halcyon/Desktop/vistorian dashboard/\"\n",
    "\n",
    "print(os.system(\"pwd\"))\n",
    "\n",
    "input_directory = os.path.join(base_path, \"user_timeline/export\")\n",
    "export_directory = os.path.join(base_path, \"data/combined-logs\")\n",
    "\n",
    "sub_folders = []\n",
    "\n",
    "for file in os.listdir(input_directory):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    first_part = file.split('_')[0]\n",
    "\n",
    "    sub_folder = os.path.join(export_directory, first_part)\n",
    "    if not os.path.exists(sub_folder):\n",
    "        os.makedirs(sub_folder)\n",
    "\n",
    "    shutil.move(os.path.join(input_directory, file), os.path.join(sub_folder, file))\n",
    "\n",
    "    if sub_folder not in sub_folders:\n",
    "        sub_folders.append(sub_folder)\n",
    "\n",
    "for sub_folder in sub_folders:\n",
    "    csv_files = [f for f in os.listdir(sub_folder) if f.endswith('.csv')]\n",
    "\n",
    "    combined_csv = pd.concat([pd.read_csv(os.path.join(sub_folder, csv_file)) for csv_file in csv_files], ignore_index=True)\n",
    "\n",
    "#     session_values = combined_csv['user-session'].tolist()\n",
    "    combined_csv['userID+session'] = combined_csv.apply(lambda row: str(row['usrID']) + '_' + str(row['userSession']), axis=1)\n",
    "    session_values_with_userID = combined_csv['userID+session'].tolist()\n",
    "#     session_values = (combined_csv['usrID'] + \"_\" + combined_csv['user-session']).tolist()\n",
    "    print\n",
    "    new_column_values = []\n",
    "\n",
    "    counter = 1\n",
    "    previous_session = session_values_with_userID[0]\n",
    "\n",
    "    for session in session_values_with_userID:\n",
    "        if session != previous_session:\n",
    "            counter += 1\n",
    "        new_column_values.append(counter)\n",
    "        previous_session = session\n",
    "\n",
    "    combined_csv['session'] = new_column_values\n",
    "\n",
    "    combined_csv.to_csv(os.path.join(sub_folder, 'users-combined-update.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bf593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
